---
title: Alignment starter pack
date: 2024-06-09
lang: en-US
minutes: ?
synopsis: TODO
run-in: Last week I was talking
---

Last week I was talking with people about AI alignment.
Despite the amount of attention that machine learning is receiving nowadays,
alignment is still a niche topic.
This worries me,
because a misaligned superintelligence has the potential
to pose an existential threat to humanity,
and I think we should treat it as seriously as nuclear warfare or climate change.
In this post I want to give a few pointers
for where to learn more about AI alignment.

## What is AI alignment?

**Alignment is the process of building an AI
whose goal aligns with the goals of its creators.**
As of 2024, alignment is an unsolved problem.
We have techniques for building <abbr>AI</abbr> whose _output_
matches what its human creators want to see on a range of inputs,
but due to reasons highlighted below,
this does not imply that its _goal_ matches that of its creators.

Recommended resource:

* [**A<!---->I Alignment: Why Itâ€™s Hard, and Where to Start**][stanford-talk]
  by Eliezer Yudkowsky.
  Keep in mind, this talk is from 2016.
  It predates the seminal 2017 paper [Attention is All You Need][attention]
  that kicked off the current wave of <abbr>LLM</abbr>s.
  The talk is still very relevant,
  but simultaneously an illustration of how quickly the field is developing.

[stanford-talk]: https://www.youtube.com/watch?v=EUjc1WuyPT8
[attention]:     https://arxiv.org/abs/1706.03762
